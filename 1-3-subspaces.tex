\section{Subspaces} \label{sec 1.3}

In the study of any algebraic structure, it is of interest to examine \emph{subsets} that \emph{possess the same structure} as the set under consideration.
The appropriate notion of substructure for vector spaces is introduced in this section.

\begin{definition} \label{def 1.2}
A \emph{subset} \(\W\) of a vector space \(\V\) over a field \(F\) is called a \textbf{subspace} of \(\V\) if \(\W\) is a vector space over \(F\) with the operations of addition and scalar multiplication \emph{defined on} \(\V\).
\end{definition}

\begin{note}
In any vector space \(\V\), note that \(\V\) itself and \(\{ \OV \}\) are subspaces (see \EXEC{1.2.11}).
The latter is called the zero subspace of \(\V\).
\end{note}

Fortunately it is \emph{not} necessary to verify all of the vector space properties to prove that a subset is a subspace.
Because properties \DEF{1.1} (VS 1), (VS 2), (VS 5), (VS 6), (VS 7), and (VS 8) hold for \emph{all} vectors in the vector space, these properties automatically hold for the vectors in any subset.
(E.g. suppose arbitrary \(w_1, w_2 \in \W\) where \(\W \subseteq \V\).
In particular \(w_1, w_2 \in \V\), so by \BLUE{(VS 1) of \(\V\)}, \(w_1 + w_2 = w_2 + w_1\).
Since \(w_1, w_2\) are arbitrary, for all \(w_1, w_2 \in \W\), \(w_1 + w_2 = w_2 + w_1\), hence \GREEN{(VS 1) of \(\W\)} is satisfied.)
Thus a subset \(\W\) of a vector space \(\V\) is a subspace of \(\V\) is and only if the following(remaining) four properties hold:
\begin{enumerate} \label{discussion of thm 1.3}
    \item \(x + y \in \W\) whenever \(x \in \W\) and \(y \in \W\). (\(\W\) is \textbf{closed under addition}.)
    \item \(cx \in \W\) whenever \(c \in F\) and \(x \in \W\). (\(\W\) is \textbf{closed under scalar multiplication}.)
    \item \(\W\) has a zero vector(VS 3).
    \item Each vector in \(\W\) has an additive inverse in \(\W\) (VS 4).
\end{enumerate}

The next theorem shows that the zero vector of \(\W\) \emph{must be the same} as the zero vector of \(\V\) and that (VS 4) is \emph{redundant}.

\begin{theorem} \label{thm 1.3}
Let \(\V\) be a vector space and \(\W\) a subset of \(\V\).
Then \(\W\) is a subspace of \(\V\) if and only if the following \emph{three} conditions hold for the operations defined in \(\V\).
\begin{enumerate}
    \item \(0 \in \W\); or more unambiguously, \(\OV \in \W\) where \(\OV\) is the zero vector of \(\V\).
    \item \(x + y \in \W\) whenever \(x \in \W\) and \(y \in \W\).
    \item \(cx \in \W\) whenever \(c \in F\) and \(x \in \W\).
\end{enumerate}
\end{theorem}

\begin{proof}
If \(\W\) is a subspace of \(\V\), then by \DEF{1.2} \(\W\) is also a vector space with the operations of addition and scalar multiplication defined on \(\V\).
Hence condition (b) and (c) hold, and by \BLUE{(VS 3) of \(\W\)}, there exists a vector \(\OW \in \W\) \MAROON{(1)} such that \(x + \OW = x\) for each \(x \in \W\).
(BTW, \(\OW\) of course belongs to \(\V\).)
But also \(x + \OV = x\), by \GREEN{(VS 3) of \(\V\)}, and thus we have \(x + \OW = x + \OV\), and thus by \THM{1.1}, we have \(\OW = \OV\).
So by \MAROON{(1)} \(\OV = \OW \in \W\), so the condition (a) holds.

Conversely, if conditions (a), (b), and (c) hold, the discussion  (\ref{discussion of thm 1.3}) preceding this theorem shows that \(\W\) is a subspace of \(\V\) if the \emph{additive inverse of each vector} in \(\W\) lies in \(\W\).
But if arbitrary \(x \in \W\), then \((-1)x \in \W\) by condition (c), and \(-x = (-1)x\) by \THM{1.2}(b)
(seriously, \THM{1.2}(b) of the \emph{vector space} \V, since in this case we do not know \(\W\) is actually a vector space yet.)
, so the additive inverse of arbitrary \(x\) lies in \(\W\).
Hence \(\W\) is a subspace of \(\V\).
\end{proof}

\begin{note}
\THM{1.3} 隱含的意義就是，每個元的素反元素會因為\ scalar 乘法有封閉性而自動存在；還有，\(\W\) 的零向量就是\ \(\V\) 的零向量。
\end{note}

\begin{additional definition} \label{adef 1.6}\ 

\BLUE{(1)} The \textbf{transpose} \(A^\top\) of an \(m \X n\) matrix \(A\) is the \(n \X m\) matrix obtained from \(A\) by \emph{interchanging the rows with the columns};
that is, \((A^\top)_{ij} = A_{ji}\).
For example,
\[
\begin{pmatrix}
1 &           -2 & \phantom{-}3 \\
0 & \phantom{-}5 &           -1
\end{pmatrix}^\top =
\begin{pmatrix}
\phantom{-}1  & \phantom{-}0 \\
          -2  & \phantom{-}5 \\
\phantom{-}3  &           -1
\end{pmatrix}
\]
and
\[
\begin{pmatrix}
1 &  2 \\
2 &  3
\end{pmatrix}^\top =
\begin{pmatrix}
 1  &  2 \\
 2  &  3
\end{pmatrix}
\]

\BLUE{(2)} A \textbf{symmetric matrix} is a matrix \(A\) such that \(A^\top = A\).
For example, the \(2 \X 2\) matrix displayed above is a symmetric matrix.
\emph{Clearly, a symmetric matrix must be square}.
\end{additional definition}

\begin{additional theorem} \label{athm 1.1}
The set \(\W\) of all symmetric matrices in \(M_{n \X n}(F)\) is a \emph{subspace} of \(M_{n \X n}(F)\) since the conditions of \THM{1.3} hold.
\end{additional theorem}

\begin{proof}
First, it is easily proved that for any matrices \(A\) and \(B\) and any scalars \(a\) and \(b\), \((aA + bB)^\top = aA^\top + bB^\top\). \MAROON{(1)} (See \EXEC{1.3.3}.)
Using this fact, we show that the set of symmetric matrices is \emph{closed} under addition and scalar multiplication.

\begin{enumerate}
    \item The zero matrix is equal to its transpose and hence belongs to \(\W\).
    \item If \(A \in \W\) and \(B \in \W\), then by definition of \(\W\), \(A^\top = A\) and \(B^\top = B\) \MAROON{(2)}.
          Thus
          \begin{align*}
              (A + B)^\top & = A^\top + B^\top & \text{by \MAROON{(1)}, with \(a, b = 1 \in F\)} \\
                        & = A + B, & \text{by \MAROON{(2)}}
          \end{align*}
          so again by definition of \(\W\), \(A + B \in \W\).
    \item And for any \(a \in F\), given \(A\) in \MAROON{(2)}, we have
        \begin{align*}
            (aA)^\top & = aA^\top & \text{by \MAROON{(1)}, with \(B\) as zero matrix} \\
                   & = aA. & \text{by \MAROON{(2)}}
        \end{align*}
        Thus by definition of \(\W\), \(aA \in \W\).
\end{enumerate}
\end{proof}

\begin{note}
The examples that follow provide further illustrations of the concept of a subspace. \emph{The first three are particularly important}.
\end{note}

\begin{example} \label{example 1.3.1}
Let \(n\) be a nonnegative integer, and let \(P_n(F)\) consist of all polynomials in \(P(F)\) \emph{having degree less than or equal to \(n\)}.
Since the zero polynomial has degree \(-1\), it is in \(P_n(F)\).
Moreover, the sum of two polynomials with degrees less than or equal to \(n\) is another polynomial of degree less than or equal to \(n\), and the product of a \emph{scalar} and a polynomial of degree less than or equal to \(n\) is a polynomial of degree less than or equal to \(n\).
So \(P_n(F)\) is \emph{closed} under addition and scalar multiplication.
It therefore follows from \THM{1.3} that \(P_n(F)\) is a subspace of \(P(F)\).
\end{example}

\begin{example} \label{example 1.3.2}
Let \(\CONTR\) denote the set of all \emph{continuous} real-valued functions defined on \(\SET{R}\).
Clearly \(\CONTR\) is a subset of the vector space \(\FRR\) defined in \EXAMPLE{1.2.3}.
We claim that \(\CONTR\) is a \emph{subspace} of \(\FRR\).

First note that the \emph{zero} of \(\FRR\) is the \emph{constant function} defined by \(f(t) = 0\) for all \(t \in \SET{R}\).
Since constant functions are continuous(by Calculus), we have \(f \in \CONTR\).
Moreover, the sum of two continuous functions is continuous, and the product of a real number and a continuous function is continuous.
So \(\CONTR\) is closed under addition and scalar multiplication and hence is a subspace of \(\FRR\) by \THM{1.3}.
\end{example}

\begin{additional definition} \label{adef 1.7}
Two special types of matrices are frequently of interest.

\BLUE{(1)} An \(m \X n\) matrix \(A\) is called \textbf{upper triangular} if all its entries lying below the \emph{diagonal} entries are zero,
that is, if \(A_{ij} = 0\) whenever \(i > j\).

\BLUE{(2)} An \(n \X n\) matrix (i.e. square) \(M\) is called a \textbf{diagonal matrix} if \(M_{ij} = 0\) whenever \(i \ne j\),
that is, if all its nondiagonal entries are zero.
For example, if
\[
A = 
\begin{pmatrix}
1 & 2 & 3 & 4 \\
0 & 5 & 6 & 7 \\
0 & 0 & 8 & 9
\end{pmatrix}
\]
and
\[
B =
\begin{pmatrix}
3 & \phantom{-}0 & 0 \\
0 &           -2 & 0 \\
0 & \phantom{-}0 & 8
\end{pmatrix},
\]
then \(A\) is an upper triangular \(3 \X 4\) matrix, and \(B\) is a \(3 \X 3\) diagonal matrix.
\end{additional definition}

\begin{example} \label{example 1.3.3}
Clearly the zero matrix is a diagonal matrix because all of its entries are \(0\).
Moreover, if \(A\) and \(B\) are diagonal \(n \X n\) matrices, then whenever \(i \ne j\),
\begin{align*}
    (A + B)_{ij} & = A_{ij} + B_{ij} & \text{by def in \EXAMPLE{1.2.2}} \\
                 & = 0 + 0 = 0 & \text{by \ADEF{1.7}(2) and \(i \ne j\)}
\end{align*}
and
\begin{align*}
    (cA)_{ij} & = cA_{ij} & \text{by def in \EXAMPLE{1.2.2}} \\
              & = c 0 = 0 & \text{by \ADEF{1.7}(2) and \(i \ne j\)}
\end{align*}
for any scalar \(c\).
Hence \(A + B\) and \(cA\) are diagonal matrices for any scalar \(c\).
Therefore the set of diagonal matrices is a subspace of \(M_{n \X n}(F)\) by \THM{1.3}.
\end{example}

\begin{example} \label{example 1.3.4}
The \textbf{trace} of an \(n \X n\) matrix \(M\), denoted \(\TRACE(M)\), is the sum of the diagonal entries of \(M\);
that is,
\[
    \TRACE(M) = M_{11} + M_{22} + ... + M_{nn}.
\]
It follows from \EXEC{1.3.6} that the set of \(n \X n\) matrices having \emph{trace equal to zero} is a subspace of \(M_{n \X n}(F)\).
\end{example}

\begin{example} \label{example 1.3.5}
The set of matrices in \(M_{m \X n}(\SET{R})\) having \emph{nonnegative} entries is \emph{not} a subspace of \(M_{m \X n}(\SET{R})\) because it is not closed under scalar multiplication (by negative scalars).
\end{example}

\begin{theorem} \label{thm 1.4}
Any \emph{intersection} of subspaces of a vector space \(\V\) (over a field \(F\)) is a subspace of \(\V\).
\end{theorem}

\begin{proof}
Proof. Let \(\mathcal{C}\) be an arbitrary collection of \emph{subspaces} of \(\V\), and let \(\W\) denote the intersection of the subspaces in \(\mathcal{C}\).
Since every subspace contains the \emph{zero vector} (of \(\V\)), \(\OV\) is in the intersection of these subspaces, i.e. \(\OV \in \W\).
Let \(a \in F\) and \(x, y \in \W\).
Then \(x\) and \(y\) are contained in \emph{each} subspace in \(C\). Because each subspace in \(\mathcal{C}\) is closed under addition and scalar multiplication, it follows that \(x + y\) and \(ax\) are contained in \emph{each} subspace in \(C\).
Hence \(x + y\) and \(ax\) are also contained in \(\W\).
So the three condition in \THM{1.3} are satisfied, hence \(\W\) is a subspace of \(\V\).
\end{proof}

Having shown that the intersection of subspaces of a vector space \(\V\) is a subspace of \(\V\), it is natural to consider whether or not the \emph{union} of subspaces of \(\V\) is a subspace of \(\V\).
It is easily seen that the union of subspaces must contain the zero vector and be closed under scalar multiplication, but in general the union of subspaces of \(\V\) need \emph{not} be closed under \emph{addition}.
In fact, it can be readily shown that the union of two subspaces of \(\V\) is a subspace of \(\V\) \textbf{if and only if} one of the subspaces \emph{contains the other}. (See \EXEC{1.3.19}.)
There is, however, a natural way(i.e. the \emph{sum}, see \ADEF{1.8}, \ADEF{1.9})  to combine two subspaces \(W_1\) and \(W_2\) to obtain a subspace that contains both \(W_1\) and \(W_2\).
This idea is explored in \EXEC{1.3.23}.
