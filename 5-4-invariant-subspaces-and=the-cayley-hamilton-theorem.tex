\section{Invariant Subspaces and the Cayley-Hamilton Theorem} \label{sec 5.4}

In \SEC{5.1}, we observed that if \(v\) is an eigenvector of a linear operator \(\T\), then \(\T\) maps the span of \(\{ v \}\) into itself.
Subspaces that are mapped into themselves are of great importance in the study of linear operators (see, e.g., \EXEC{2.1.29}--\EXEC{2.1.33}).

\begin{definition} \label{def 5.14}
Let \(\T\) be a linear operator on a vector space \(\V\).
A subspace \(\W\) of \(\V\) is called a \textbf{\(\T\)-invariant subspace} of \(\V\) if \(\T(\W) \subseteq \W\), that is, if
\(\T(v) \in \W\) for all \(v \in \W\).
\end{definition}

\begin{example} \label{example 5.4.1}
Suppose that \(\T\) is a linear operator on a vector space \(\V\).
Then the following subspaces of \(\V\) are \(\T\)-invariant:
\begin{enumerate}
\item[1.] \(\{ \OV \}\)
\item[2.] \(\V\)
\item[3.] \(\RANGET\)
\item[4.] \(\NULLT\)
\item[5.] \(E_{\lambda}\), for any eigenvalue \(\lambda\) of \(\T\).
\end{enumerate}

For 5., see \EXEC{5.4.3}, for other items, see \EXEC{2.1.29}.
\end{example}

\begin{example} \label{example 5.4.2}
Let \(\T\) be the linear operator on \(\SET{R}^3\) defined by
\[
    \T(a, b, c) = (a + b, b + c, 0).
\]
Then the \(xy\)-plane = \(\{ (x, y, 0): x, y \in \SET{R} \}\) and the \(x\)-axis = \(\{ (x, 0, 0): x \in \SET{R}\}\) are (some examples of) \(\T\)-invariant subspaces of \(\SET{R}^3\).
\end{example}

\begin{additional definition} \label{adef 5.5}
Let \(\T\) be a linear operator on a vector space \(\V\), and let \(x\) be a \emph{nonzero} vector in \(\V\).
The subspace \(\W = \spann(\{ x, \T(x), \T^2(x), ... \})\) is called the \textbf{\(\T\)-cyclic subspace of \(\V\) generated by \(x\)}.
It is a simple matter to show that \(\W\) is \(\T\)-invariant.

Note that we do not require \(\V\) to be finite-dimensional.
\end{additional definition}

\begin{remark} \label{remark 5.4.1}
In fact, \(\W\) is the ``smallest'' \(\T\)-\emph{invariant subspace} of \(\V\) containing \(x\).
That is, any \(\T\)-invariant subspace of \(\V\) containing \(x\) must also contain \(\W\) (see \EXEC{5.4.11}).
Cyclic subspaces have various uses.
We apply them in this section to establish the Cayley Hamilton theorem.
In \EXEC{5.4.31}, we outline a method for using cyclic subspaces to \emph{compute the \CPOLY{}} of a linear operator \textbf{without resorting to determinants}.
Cyclic subspaces also play an important role in \CH{7}, where we study matrix representations of non-diagonalizable linear operators.
\end{remark}

\begin{example} \label{example 5.4.3}
Let \(\T\) be the linear operator on \(\SET{R}^3\) defined by
\[
    \T(a, b, c) = (-b + c, a + c, 3c).
\]
We determine the \(\T\)-cyclic subspace generated by \(e_1 = (1, 0, 0)\).
Since
\[
    \T(e_1) = \T(1, 0, 0) = (0, 1, 0) = e_2
\]
and
\[
    \T^2(e_1) = \T(\T(e_1)) = \T(e_2) = (-1, 0, 0) = -e_1,
\]
it follows that
\[
    \spann(\{ e_1, \T(e_1), \T^2(e_1), ... \}) = \spann(\{ e_1, e_2 \}) = \{ (s, t, 0) : s, t \in \SET{R} \}.
\]
\end{example}

\begin{example} \label{example 5.4.4}
Let \(\T\) be the linear operator on \(\mathcal{P}(\SET{R})\) defined by \(\T(f(x)) = f'(x)\).
Then the \(\T\)-cyclic subspace generated by \(x^2\) is (of course!) \(\spann(\{ x^2, 2x, 2 \}) = \mathcal{P}_2(\SET{R})\).
\end{example}

\begin{remark} \label{remark 5.4.2}
The existence of a \(\T\)-invariant subspace provides the opportunity to define a new linear operator \textbf{whose domain is this subspace}.
If \(\T\) is a linear operator on \(\V\) and \(\W\) is a \(\T\)-invariant subspace of \(\V\), then the \emph{restriction} \(\T_W\) of \(\T\) to \(\W\) (see Appendix B, page 545) is a mapping \emph{from \(\W\) to \(\W\)}, and it follows that \(\T_W\) is a linear operator on \(\W\) (see \EXEC{5.4.7}, or \EXEC{2.1.30}).
As a linear operator, \(\T_W\) \emph{inherits certain properties} from its parent operator \(\T\).
The following result illustrates one way in which the two operators are linked.
\end{remark}

\begin{theorem} \label{thm 5.20}
Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\), and let \(\W\) be a \(\T\)-invariant subspace of \(\V\).
Then the \CPOLY{} of \(\T_W\) \textbf{divides} the \CPOLY{} of \(\T\).
\end{theorem}

\begin{proof}
Choose an ordered basis \(\gamma = \{ v_1, v_2, ..., v_k \}\) \emph{for \(\W\)}, and \emph{extend} it to an ordered basis \(\beta = \{ v_1, v_2, ... v_k, v_{k + 1}, ..., v_n \}\) for \(\V\).
Let \(A = [\T]_{\beta}\) and \(B_1 = [\T_W]_{\gamma}\).
Then, by \EXEC{5.4.12}, \(A\) can be written in the form
\[
    A = \begin{pmatrix}
        \RED{B_1} & B_2 \\ O & B_3
    \end{pmatrix}.
\]
Let \(f(t)\) be the \CPOLY{} of \(\T\) and \(g(t)\) the \CPOLY{} of \(\T_W\).
Then
\[
    f(t) = \det(A - tI_n) = \begin{pmatrix} B_1 - t I_{\RED{k}} & B_2 \\ O & B_3 - t I_{\RED{n - k}} \end{pmatrix}
    = g(t) \cdot \det(B_3 - t I_{n - k})
\]
by \EXEC{4.3.21}.
Thus \(g(t)\) divides \(f(t)\).
\end{proof}

\begin{example} \label{example 5.4.5}
Let \(\T\) be the linear operator on \(\SET{R}^4\) defined by
\[
    \T(a, b, c, d) = (a + b + 2c - d, b + d, 2c - d, c + d),
\]
and let \(\W = \{ (t, s, 0, 0) : t, s \in \SET{R} \}\).
Observe that \(\W\) is a \(\T\)-invariant subspace of \(\SET{R}^4\) because, for any vector \((a, b, 0, 0) \in \W\),
\[
    \T(a, b, 0, 0) = (a + b, b, 0, 0) \in \W.
\]
Let \(\gamma = \{ e_1, e_2 \}\), which is an ordered basis for \(\W\).
Extend \(\gamma\) to the \emph{standard} ordered basis \(\beta\) for \(\SET{R}^4\).
Then
\[
    B_1 = [\T_W]_{\gamma} = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}
    \quad \text{ and } \quad
    A = [\T]_{\beta} = \begin{pmatrix} 1 & 1 & 2 & -1 \\ 0 & 1 & 0 & 1 \\ 0 & 0 & 2 & -1 \\ 0 & 0 & 1 & 1 \end{pmatrix}
\]
in the notation of \THM{5.20}.
Let \(f(t)\) be the \CPOLY{} of \(\T\) and \(g(t)\) be the \CPOLY{} of \(\T_W\).
Then
\begin{align*}
    f(t) & = \det(A - tI_4) = \det \begin{pmatrix} 1-t & 1 & 2 & -1 \\ 0 & 1-t & 0 & 1 \\ 0 & 0 & 2-t & -1 \\ 0 & 0 & 1 & 1-t \end{pmatrix} & \text{by definition} \\
         & = \det \begin{pmatrix} 1-t & 1 \\ 0 & 1-t \end{pmatrix} \cdot \det \begin{pmatrix} 2-t & -1 \\ 1 & 1-t \end{pmatrix} & \text{by \EXEC{4.3.21}} \\
         & = g(t) \cdot \det \begin{pmatrix} 2-t & -1 \\ 1 & 1-t \end{pmatrix} & \text{by \THM{5.20}}
\end{align*}
\end{example}

\begin{remark} \label{remark 5.4.3}
In view of \THM{5.20}, we may use the \CPOLY{} of \(T_W\) to \emph{gain information} about the \CPOLY{} \emph{of \(\T\) itself}.
In this regard, cyclic subspaces are useful because the \CPOLY{} of the restriction of a linear operator \(\T\) to a cyclic subspace is \emph{readily computable}.
(See the theorem below.)
\end{remark}

\begin{theorem} \label{thm 5.21}
Let \(\T\) be a linear operator on a \textbf{finite}-dimensional vector space \(\V\), and let \(\W\) denote the \(\T\)-cyclic subspace of \(\V\) generated by a nonzero vector \(v \in \V\).
Let \(k = \dim(\W)\).
Then
\begin{enumerate}
\item \(\beta = \{ v, \T(v), \T^2(v), ..., \T^{\RED{k - 1}}(v) \}\) is a \textbf{basis} for \(\W\).
\item If \(\T_{\RED{k}}(v) = -a_0 v - a_1 \T(v) - ... - a_{k - 1} \T^{k - 1}(v) \), then the \CPOLY{} oF \(\T_W\) is \(f(t) = (-1)^k (t^k + a_{k - 1} t^{k-1} + ... + a_1 t + a_0).\).

(That is, If we write \(\T_{\RED{k}}(v)\) as a linear combination \(\beta\), but without loss of generality and a negative sign in these coefficient \(a_i\), we can derive the \CPOLY{} of \(\T_W\) use these coefficients \(a_i\).)
\end{enumerate}
\end{theorem}

\begin{proof} \ 

\begin{enumerate}
\item Since \(v \ne \OV\), the set \(\{ v \}\) is \LID{}.
Let \(j\) be the \emph{largest positive} integer for which
\[
    \beta = \{ v, \T(v), ..., \T^{j - 1}(v) \}
\]
is linearly independent.
Such \(j\) must exist since \(\V\) is finite-dimensional (hence any \LID{} set must be finite).
Let \(Z = \spann(\beta)\).
Then \(\beta\) is a generating set for \(Z\) and is \LID{}, hence is a basis for \(Z\).
Furthermore, since by definition of \(j\), \(\beta \cup \{ \T^j(v) \}\) is \LDP{}, \(\T^j(v) \in \spann(\beta)\) by \THM{1.7}.
That is, \(\T^j(v) \in Z\).
We use this information to show that \(Z\) is a \(\T\)-invariant subspace of \(\V\).

Let \(w \in Z\).
Since \(w\) is a linear combination of the vectors of \(\beta\), there exist scalars \(b_0, b_1, ..., b_{j - 1}\) such that
\[
    w = b_0 v + b_1 \T(v) + ... + b_{j - 1} \T^{j - 1}(v),
\]
and hence (by applying \(\T\) on both sides,)
\begin{align*}
    \T(w) & = \T(b_0 v + b_1 \T(v) + ... + b_{j - 1} \T^{j - 1}(v)) \\
          & = b_0 \T(v) + b_1 \T^2(v) + ... + b_{j - 2} \T^{\RED{j - 1}}(v) + b_{j - 1} \T^{\RED{j}}(v), & \text{since \(\T\) is linear}
\end{align*}
where \(\T(v), ..., \T^{\RED{j - 1}}(v)\) belong to the bases \(\beta\), and \(\T^{\RED{j}}(v) \in Z\), thus \(\T(w)\) is still a linear combination of vectors in \(Z\), and hence belong to \(Z\).
So \(Z\) is \(\T\)-invariant.
Furthermore, \(v \in Z\) (since \(v \in \beta\)).
By \EXEC{5.4.11}, \(\W\) is the \emph{smallest} \(\T\)-invariant subspace of \(\V\) that contains \(v\), so that \(\W \subseteq Z\).
Clearly, \(Z \subseteq \W\) (since \(Z = \spann(\{ v, \T(v), ..., \T^{j - 1}(v) \})\) but \(\W = \spann(\{ v, \T(v), ... \})\)), and so we conclude that \(Z = \W\).
It follows that \(\beta\) is a basis for \(\W\), and therefore \(\dim(\W) = j\).
Thus \(j = k\).
This proves (a).

\item Now view \(\beta\) (from part(a)) as an ordered basis for \(\W\).
Let \(a_o, a_1, ..., a_{k_1}\) be the scalars such that
\[
    a_0 v + a_1 \T(v) + ... + a_{k - 1} \T^{k - 1} (v) + \RED{\T^k(v)} = \OV. \quad \quad \quad \MAROON{(1)}
\]
(Note that this is \emph{not} a linear combination of vectors in \(\beta\).)
Observe that since
\begin{align*}
    \T(v) & = 0 \cdot v + 1 \cdot \T(v) + 0 \cdot \T^2(V) + ... + 0 \cdot \T^{k - 1}(v) \\
    \T(\T(v)) & = 0 \cdot v + 0 \cdot \T(v) + 1 \cdot \T^2(V) + ... + 0 \cdot \T^{k - 1}(v) \\
    \vdots \\
    \T(\T^{k - 2})(v) & = 0 \cdot v + 0 \cdot \T(v) + 0 \cdot \T^2(V) + ... + 1 \cdot \T^{k - 1}(v) \\
    \T(\T^{k - 1}(v)) & = \T^k(v) = -a_0 \cdot v -a_1 \cdot \T(v) - a_2 \cdot \T^2(v) - ... - a_{k - 1} \T^{k - 1}(v),
\end{align*}
We have
\[
    [\T_W]_{\beta} = \begin{pmatrix}
        0 & 0 & 0 & ... & 0 & -a_0 \\
        1 & 0 & 0 & ... & 0 & -a_1 \\
        0 & 1 & 0 & ... & 0 & -a_2 \\
        \vdots & \vdots & \ddots & & 0 & \vdots \\
        0 & 0 & 0 & & 1 & -a_{k - 1}
    \end{pmatrix},
\]
which has the \CPOLY{}
\[
    f(t) = (-1)^k (t^k + a_{k - 1} t^{k-1} + ... + a_1 t + a_0)
\]
by \EXEC{5.4.19}.
Thus \(f(t)\) is the \CPOLY{} of \(\T_W\), proving (b).
\end{enumerate}
\end{proof}

\begin{example} \label{example 5.4.6}
Let \(\T\) be the linear operator of \EXAMPLE{5.4.3}, and let \(\W = \spann(\{ e_1, e_2 \})\), the \(\T\)-cyclic subspace generated by \(e_1\).
We compute the characteristic polynomial \(f(t)\) of \(\T_W\) in two ways: by means of \THM{5.21} and by means of determinants.
\begin{enumerate}
\item \emph{By means of \THM{5.21}}.
From \EXAMPLE{5.4.3}, we know that \(\{ e_1, e_2\}\) is a \textbf{cycle}(\TODOREF{this term seem to be defined in \CH{7}}) that generates \(\W\), hence \(\dim(\W) = \RED{2}\), hence from \THM{5.21}(a), \(\{ e_1, \T(e_1) \}\) is a basis for \(\W\);
in particular, by that example, that set is in fact equal to \(\{ e_1, e_2 \}\).
Also from that example, we have \(\T^{\RED{2}}(e_1) = -e_1\).
And from this equation, we have
\[
    1 \cdot e_1 + 0 \cdot \T(e_1) + \T^{\RED{2}}(e_1) = 0.
\]
Therefore, by \THM{5.21}(b),
\[
    f(t) = (-1)^{\RED{2}} (1 + 0t + t^2) = t^2 + 1.
\]

\item By means of determinants, Let \(\beta = \{ e_1, e_2 \}\), which (by that example) is an ordered basis for \(\W\).
Since \(\T(e_1) = e_2\) and \(\T(e_2) = -e_1\), we have
\[
    [\T_W]_{\beta} = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}.
\]
and therefore
\[
    f(t) = \det \begin{pmatrix} -t & -1 \\ 1 & -t \end{pmatrix} = t^2 + 1.
\]
\end{enumerate}
\end{example}

\subsection{The Cayley-Hamilton Theorem} \label{sec 5.4.1}
As an illustration of the importance of \THM{5.21}, we prove a well-known result that is used in \CH{7}.
The reader should refer to \DEF{e.3} for the definition of \(f(\T)\), where \(\T\) is a linear operator and \(f(x)\) is a polynomial.

\begin{theorem} [Cayley-Hamilton] \label{thm 5.22}
Let \(\T\) be a linear operator on a \emph{finite}-dimensional vector space \(\V\), and let \(f(t)\) be the \CPOLY{} of \(\T\).
Then \(f(\T) = \TZERO\), \textbf{the zero transformation}.
That is, \(\T\) ``satisfies'' its characteristic equation.
\end{theorem}

\begin{proof}
We show that \(f(\T)(v) = \OV\) for all \(v \in \V\).
This is obvious if \(v = \OV\) because \(f(\T)\) is linear(see \THM{e.3}(a)); so suppose that \(v \ne \OV\).
Let \(\W\) be the \(\T\)-cyclic subspace generated by \(v\), and suppose that \(\dim(\W) = k\).
By definition of \(\T\)-cyclic, \(\T^k(v) \in \W\).
And by \THM{5.21}(a), \(\{ v, \T(v), ..., \T^{k - 1}(v) \}\) is a basis for \(\W\), hence there exist scalars \(a_0, a_1, ..., a_{k-1}\) such that
\[
    a_0 v + a_1 \T(v) + ... + a_{k - 1} \T^{k - 1}(v) = -\RED{\T^k(v)},
\]
or
\[
    a_0 v + a_1 \T(v) + ... + a_{k - 1} \T^{k - 1}(v) + \RED{\T^k(v)} = \OV. \quad \quad \quad \MAROON{(1)}
\]
Hence, \THM{5.21}(b) implies that
\[
    g(t) = (-1)^k (a_0 + a_1 t + ... + a_{k - 1} t^{k - 1} + t^k) \quad \quad \quad \MAROON{(2)}
\]
is the \CPOLY{} of \(\T_W\).
Combining these two equations yields
\begin{align*}
    g(\T)(v) & = (-1)^k (a_0 \ITRANV{} + a_1 \T + ... + a_{k - 1} \T^{k - 1} + \T^k)(v) & \text{by \DEF{e.3} and \MAROON{(2)}} \\
        & = (-1)^k [ a_0 \ITRANV{}(v) + a_1\T(v) + ... + a_{k - 1} \T^{k - 1}(v) + \T^k(v) ] & \text{since function \(+, \cdot\) is linear} \\
        & = (-1)^k [ a_0 v + a_1\T(v) + ... + a_{k - 1} \T^{k - 1}(v) + \T^k(v) ] & \text{of course} \\
        & = (-1)^k \cdot \OV & \text{by \MAROON{(1)}} \\
        & = \OV. \quad \MAROON{(3)} & \text{of course}
\end{align*}
By \THM{5.20}, \(g(t)\) divides \(f(t)\); hence there exists a polynomial \(q(t)\) such that \(f(t) = q(t)g(t)\). \MAROON{(4)}

So
\begin{align*}
    f(\T)(v) & = q(\T)g(\T)(v) & \text{by \MAROON{(4)} and \DEF{e.3}} \\
             & = q(\T)(g(\T)(v)) & \text{by def of function composition} \\
             & = q(\T)(\OV) & \text{by \MAROON{(3)}} \\
             & = \OV. & \text{since \(q(\T)\) is also linear by \THM{e.3}(a)}
\end{align*}
So in all cases, \(f(\T)(v) = \OV\) for arbitrary \(v\), as desired.
\end{proof}

\begin{example} \label{example 5.4.7}
Let \(\T\) be the linear operator on \(\SET{R}^2\) defined by \(\T(a, b) = (a + 2b, -2a + b)\), and let \(\beta = \{ e_1, e_2 \}\).
Then
\begin{align*}
    A = \begin{pmatrix} 1 & 2 \\ -2 & 1 \end{pmatrix},
\end{align*}
where \(A = [\T]_{\beta}\).
The \CPOLY{} of \(\T\) is, therefore
\[
    f(t) = \det(A - tI) = \det \begin{pmatrix} 1-t & 2 \\ -2 & 1-t \end{pmatrix} = t^2 - 2t + 5.
\]
It is easily verified that \(\TZERO = f(\T) = \T^2 - 2\T + 5\ITRANV{}\).
Similarly,
\[
    f(A) = A^2 - 2A + 5I =
    \begin{pmatrix} -3 & 4 \\ -4 & -3 \end{pmatrix}
    + \begin{pmatrix} -2 & -4 \\ 4 & -2 \end{pmatrix}
    + \begin{pmatrix} 5 & 0 \\ 0 & 5 \end{pmatrix}
    = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}.
\]
\end{example}

\EXAMPLE{5.4.7} suggests the following result.

\begin{corollary} [Cayley-Hamilton Theorem for Matrices] \label{corollary 5.22.1}
Let \(A\) be an \(n \X n\) matrix, and let \(f(t)\) be the \CPOLY{} of \(A\).
Then \(f(A) = O\), the \(n \X n\) zero matrix.
\end{corollary}

\begin{proof}
We have to show \(f(A) v = \OV\) for all \(v \in F^n\) and conclude \(f(A) = O\).
Let \(\beta\) be the \emph{standard} ordered basis for \(F^n\), then by \THM{2.15}(a), \([\LMTRAN_A]_{\beta} = A\), and (by \DEF{5.4}) \(f(t)\) is equal to the \CPOLY{} of \(\LMTRAN_A\).
Without loss of generality, let \(f(t) = a_0 + a_1 t + ... + a_n t^n\). \MAROON{(1)}

We have
\begin{align*}
    f(A) v & = [a_0 I + a_1 A + ... + a_n A^n](v) & \text{by \MAROON{(1)} and \DEF{e.3}} \\
           & = (a_0 I)(v) + (a_1 A)(v) + ... + (a_n A^n)(v) & \text{by \THM{2.12}} \\
           & = a_0 [\ITRANV{}]_{\beta} (v) + a_1 [\LMTRAN_A]_{\beta} (v) + ... + a_n [(\LMTRAN_A)^n] (v) & \text{by \THM{2.15} and \THM{2.11}} \\
           & = [\left( a_0 \ITRANV{} + a_1 \LMTRAN_A + ... + a_n (\LMTRAN_A)^n \right) (v)]_{\beta} & \text{just by \CH{2}, :P} \\
           & = [\TZERO(v)]_{\beta} = [\OV]_{\beta} & \text{by \THM{5.22}} \\
           & = \OV & \text{since \(\OV \in F^n\)}
\end{align*}
\end{proof}

\subsection{Invariant Subspaces and Direct Sums} \label{sec 5.4.2}
This subsection uses optional material on direct sums from subsection \ref{sec 5.2.3}.

It is useful to decompose a finite-dimensional vector space \(\V\) into a \emph{direct sum} of as many \(\T\)-\textbf{invariant} subspaces as possible because the behavior of \(\T\) on \(\V\) can be inferred from its behavior on the direct summands.
For example, \(\T\) is diagonalizable if and only if \(\V\) can be decomposed into a direct sum of \textbf{one-dimensional} \(\T\)-invariant subspaces (see \EXEC{5.4.35}).
In \CH{7}, we consider \emph{alternate ways} of decomposing \(\V\) into direct sums of \(\T\)-invariant subspaces if \(\T\) is not diagonalizable.
We proceed to gather a few facts about direct sums of \(\T\)-invariant subspaces that are used in \SEC{7.4}.
The first of these facts is about \CPOLY{}s.

\begin{theorem} \label{thm 5.23}
Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\), and suppose that \(\V = \W_1 \oplus \W_2 \oplus ... \oplus \W_{\RED{k}}\), where \(\W_i\) is a \(\T\)-\textbf{invariant} subspace of \(\V\) for each \(i (1 \le i \le k)\).
Suppose that \(f_i(t)\) is the \CPOLY{} of \(T_{\W_i} (1 \le i \le k)\).
Then \(f_1(t) \cdot f_2(t) \cdot ... \cdot f_k(t)\) is the \CPOLY{} of \(\T\).
\end{theorem}

\begin{proof}
The proof is by mathematical induction on \(\RED{k}\).
In what follows, \(f(t)\) denotes the \CPOLY{} of \(\T\).
Suppose first that \(k = 2\).
(That is, \(\V = \W_1 \oplus \W_2\) where \(\W_1, \W_2\) are \(\T\)-invariant subspaces of \(\V\).)
Let \(\beta_1\) be an ordered basis for \(\W_1\), \(\beta_2\) an ordered basis for \(\W_2\), and \(\beta = \beta_1 \cup \beta_2\).
Then \(\beta\) is an ordered basis for \(\V\) by \THM{5.9}(d).
Let \(A = [\T]_{\beta}\), \(B_1 = [\T_{\W_1}]_{\beta_1}\) and \(B_2 = [\T_{\W_2}]_{\beta_2}\).
By \EXEC{5.4.33}, it follows that
\[
    A = \begin{pmatrix}
        B_1 & O \\
        O' & B_2
    \end{pmatrix},
\]
where \(O\) and \(O'\) are zero matrices of the appropriate sizes.
Then, without loss of generality that each \(I\) denotes identity matrix of the appropriate size, we have
\begin{align*}
    f(t) & = \det(A - tI) & = \begin{pmatrix} B_1 - tI & O \\ O' & B_2 - tI \end{pmatrix}\\
         & = \det(B_1 - tI) \cdot det(B_2 - tI) & \text{by \EXEC{4.3.21}} \\
         & = f_1(t), \cdot f_2(t) & \text{by definition}
\end{align*}
proving the result for \(k = 2\).

Now assume that the theorem is valid for \(k - 1\) summands, where \(k - 1 \ge 2\).
(That is, given any linear operator \(\T\), and suppose that \(\V = \W_1 \oplus \W_2 \oplus ... \oplus \W_{\RED{k - 1}}\), where \(\W_i\) is a \(\T\)-\textbf{invariant} subspace of \(\V\) for each \(i (1 \le i \le k)\), and \(f_i(t)\) is the \CPOLY{} of \(T_{\W_i} (1 \le i \le k)\).
Then \(f_1(t) \cdot f_2(t) \cdot ... \cdot f_{k - 1}(t)\) is the \CPOLY{} of \(\T\).)

And suppose that \(\V\) is a direct sum of \(k\) \(\T\)-\textbf{invariant}\RED{*} subspaces, say,
\[
    \V = \W_1 \oplus \W_2 \oplus ... \oplus \W_{k - 1} \oplus \W_k.
\]
Let \(\W = \W_1 + \W_2 + ... + \W_{k - 1}\).
It is \emph{easily verified} that \(\W\) is \(\T\)-invariant and that \(\V = \W \oplus \W_k\).
So by the case for \(k = 2\), \(f(t) = g(t) \cdot f_k(t)\), where \(g(t)\) is the \CPOLY{} of \(\T_W\).

\RED{**}And clearly, \(\W_i\) are subspace of \(\W\), and furthermore, \(\W_i\) is a \(\T_{\RED{\W}}\)-invariant subspace of \(\W\).
Also, clearly, \(\W = \W_1 \oplus \W_2 ... \oplus \W_{k - 1}\).
Therefore, all conditions of induction hypothesis are satisfied, hence \(g(t) = f_1(t) \cdot f_2(t) \cdot ... \cdot f_{k - 1}(t)\) by the induction hypothesis.
Combining these facts, we have \(f(t) = g(t) \cdot f_k(t) = f_1(t) \cdot f_2(t) \cdot ... \cdot f_{k - 1}(t) \cdot f_k(t)\).
\end{proof}

\begin{note}
\RED{*}: The proof in the book for the case \(k\) does not say that \(\W_1, ..., \W_k\) are \(\T\)-invariant, but it seems that they need to be \(\T\)-invariant.

\RED{**}: In my opinion, the usage of the induction hypothesis in the book is too vague to be correct.
\end{note}

\begin{remark} \label{remark 5.4.4}
As an illustration of \THM{5.23}, suppose that \(\T\) is a diagonalizable linear operator on a \emph{finite}-dimensional vector space \(\V\) with distinct eigenvalues \(\lambda_1, \lambda_2, ..., \lambda_k\).
By \THM{5.10}, (since \(\T\) is diagonalizable,) \(\V\) is a direct sum of the \emph{eigenspaces} of \(\T\).
Since each eigenspace \emph{is} \(\T\)-invariant (see \EXAMPLE{5.4.1}), we may view this situation in the context of \THM{5.23}.
(That is, we have \(E_{\lambda_1}, ..., E_{\lambda_k}\) as \(\T\)-invariant subspaces and \(\V = E_{\lambda_1} \oplus ... \oplus E_{\lambda_k}\).
For each eigenvalue \(\lambda_i\), the \emph{restriction} of \(\T\) to \(E_{\lambda_i}\), has characteristic polynomial \((\lambda - t)^{m_i}\), where \(m_i\) is the dimension of \(E_{\lambda_i}\).
(why \((\lambda - t)^{m_i}\)? Given any basis \(\beta_i\) for \(E_{\lambda_i}\), then \([\T_{E_{\lambda_i}}]_{\beta}\) is a diagonal matrix such that each diagonal entry is \(\lambda_i\), since each vector in the basis is scaled \(\lambda_i\) times by \(\T_{E_{\lambda_i}}\).)
By \THM{5.23}, the characteristic polynomial \(f(t)\) of \(\T\) is the product
\[
    f(t) = (\lambda_1 - t)^{m_1} (\lambda_2 - t)^{m_2} ... (\lambda_k - t)^{m_k}.
\]
It follows that the algebraic multiplicity of each eigenvalue is equal to the dimension of the corresponding eigenspace, as expected.
\end{remark}

\begin{example} \label{example 5.4.8}
Let \(\T\) be the linear operator on \(\SET{R}^4\) defined by
\[
    \T(a, b, c, d) = (2a - b, a + b, c - d, c + d),
\]
and let \(\W_1 = \{(s, t, 0, 0): s,t \in \SET{R}\}\) and \(\W_2 = \{(0, 0, s, t): s,t \in \SET{R}\}\).
Notice that \(\W_1\) and \(\W_2\) are each \(\T\)-invariant and that \(\SET{R}^4 = \W_1 \oplus \W_2\).
Let \(\beta_1 = \{ e_1, e_2 \}\), \(\beta_2 = \{ e_3, e_4 \}\), and \(\beta = \beta_1 \cup \beta_2 = \{ e_1, e_2, e_3, e_4 \}\).
Then \(\beta_1\) is an ordered basis for \(\W_1\), \(\beta_2\) is an ordered basis for \(\W_2\), and \(\beta\) is an ordered basis for \(\SET{R}^4\).
Let \(A= [\T]_{\beta}, B_1 = [\T_{\W_1}]_{\beta_1}\) and \(B_2 = [\T_{\W_2}]_{\beta_2}\).
Then
\[
    B_1 = \begin{pmatrix} 2 & -1 \\ 1 & 1 \end{pmatrix}, \quad
    B_2 =\begin{pmatrix} 1 & -1 \\ 1 & 1 \end{pmatrix}
\]
and
\[
    A = \begin{pmatrix} B_1 & O \\ O & B_2 \end{pmatrix}
    = \begin{pmatrix}
        2 & -1 & 0 & 0 \\
        1 & 1 & 0 & 0 \\
        0 & 0 & 1 & -1 \\
        0 & 0 & 1 & 1
    \end{pmatrix}
\]
Let \(f(t), f_1(t)\), and \(f_{2}(t)\) denote the \CPOLY{} of \(\T, \T_{\W_1}\), and \(\T_{\W_2}\), respectively.
Then by \THM{5.23},
\[
    f(t) = \det(A - tI) = \det(B_1 - tI) \cdot \det(B_2 - tI) = f_1(t) \cdot f_2(t).
\]
\end{example}

The matrix \(A\) in \EXAMPLE{5.4.8} can be obtained by \emph{joining} the matrices \(B_1\) and \(B_2\) in the manner explained in the next definition.

\begin{definition} \label{def 5.15}
Let \(B_1 \in M_{m \X m}(F)\), and let \(B_2 \in M_{n \X n}(F)\).
We define the direct sum of \(B_1\) and \(B_2\), (that is, the ``direct sum of two matrices'') denoted \(B_1 \oplus B_2\), as the \((m + n) \X (m + n)\) matrix \(A\) such that
\begin{equation*}
    A_{ij} = \begin{cases}
        (B_1)_{ij} & \text{ for } 1 \le i, j \le m \\
        (B_2)_{(i - m), (j - m)} & \text{ for } m + 1 \le i, j \le m + n \\
        0 & \text{ otherwise. }
    \end{cases}
\end{equation*}

If \(B_1, B2, ..., B_k\) are square matrices with entries from \(F\), then we define the \textbf{direct sum} of \(B_1, B_2, ..., B_k\) \emph{recursively} by
\[
    B_1 \oplus B_2 \oplus ... \oplus B_k = (B_1 \oplus B_2 \oplus ... \oplus B_{k - 1}) \oplus B_k.
\]
If \(A = B_1 \oplus B_2 \oplus ... \oplus B_k\), then we often writ
\[
    A = \begin{pmatrix}
        B_1 & O & ... & O \\
        O   & B_2 & ... & O \\
        \vdots & \vdots & & \vdots \\
        O & O & \vdots & B_k
    \end{pmatrix}.
\]
\end{definition}

\begin{example} \label{example 5.4.9}
Let
\[
    B_1 = \begin{pmatrix} 1 & 2 \\ 1 & 1  \end{pmatrix},
    \quad B_2 = \begin{pmatrix} 3 \end{pmatrix},
    \quad \text{ and } \quad
    B_3 = \begin{pmatrix} 1 & 2 & 1 \\ 1 & 2 & 3 \\ 1 & 1 & 1 \end{pmatrix}.
\]
Then
\[
    B_1 \oplus B_2 \oplus B_3 = \begin{pmatrix}
        1 & 2 & 0 & 0 & 0 & 0 \\
        1 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 3 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 2 & 1 \\
        0 & 0 & 0 & 1 & 2 & 3 \\
        0 & 0 & 0 & 1 & 1 & 1
    \end{pmatrix}
\]
\end{example}

The final result of this section relates direct sums of matrices to direct sums of invariant subspaces.
It is an extension of \EXEC{5.4.33} to the case \(k \ge 2\).

\begin{theorem} \label{thm 5.24}
Let \(\T\) be a linear operator on a \emph{finite}-dimensional vector space \(\V\), and let \(\W_1, \W_2, ..., \W_k\) be \(\T\)-invariant subspaces of \(\V\) such that \(\V = \W_1 \oplus \W_2 \oplus ... \oplus \W_k\).
For each \(i\), let \(\beta_i\) be an ordered basis for \(\W_i\), and let \(\beta = \beta_1 \cup \beta_2 \cup \cup \beta_k\). Let \(A = [\T]_{\beta}\) and \(B_i = [\T_{\W_i}]_{\beta_i}\), for \(i = 1, 2, ..., k\).
Then \(A = B_1 \oplus B_2 \oplus ... \oplus B_k\).

(Note that \RMK{5.4.4} is a particular example for this theorem.)
\end{theorem}

\begin{proof}
See \EXEC{5.4.34}.
\end{proof}
